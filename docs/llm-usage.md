# RAGChain - LLM Architecture & Flow Diagram

## ğŸ” Where LLMs Are Called

This document identifies **ALL** locations where LLMs are invoked in the RAGChain system and provides a visual flow diagram.

---

## ğŸ¨ Visual Architecture Diagrams

### Full System Architecture

![RAGChain Architecture](images/architecture.png)

**Legend:**
- ğŸ”´ **Red Star with "LLM CALL" badge** = LLM invocation point (prompt + generate)
- âœ… **Green Checkmark with "No LLM"** = Statistical/algorithmic operation (no API call)
- ğŸ”µ **Blue** = Data ingestion pipeline
- ğŸŸ¢ **Green** = Search/retrieval operations
- ğŸŸ  **Orange** = RAG pipeline with adaptive routing
- ğŸŸ£ **Purple** = Evaluation workflow

### LLM Invocation Points & Configuration

![LLM Usage Details](images/llm-usage.png)

This diagram shows:
- **4 LLM call points** with detailed configurations (temperature, tokens, context window)
- **Query flow** showing when each LLM is invoked
- **Configuration table** comparing settings across all LLM calls
- **Cost metrics** showing per-query LLM usage (1-3 calls)
- **Conditional paths** highlighting fast-paths and retries

**Key Insights:**
- Only **1 LLM call** is required (if routing is skipped and no retry needed)
- **2 LLM calls** is typical (routing + generation)
- **3 LLM calls** maximum in production (routing + rewriting + generation)
- **Statistical operations** (grader, retriever) avoid LLM costs

---

## ğŸ“ LLM Call Locations

### 1. **Intent Router** (`src/ragchain/inference/router.py`)
- **Function**: `intent_router()`
- **Purpose**: Classify query intent (FACT/CONCEPT/COMPARISON)
- **LLM Model**: `get_llm(purpose="routing")`
- **Settings**: 
  - `temperature=0.0` (deterministic)
  - `num_predict=32` (short output)
  - `num_ctx=config.ollama_routing_ctx`
- **Prompt**: `INTENT_ROUTER_PROMPT`
- **When Called**: Start of RAG pipeline (unless fast-path detected)
- **Output**: Intent classification (FACT/CONCEPT/COMPARISON)

```python
llm = get_llm(purpose="routing")
prompt = INTENT_ROUTER_PROMPT.format(query=state["query"])
response = llm.invoke(prompt).strip().upper()
```

---

### 2. **Query Rewriter** (`src/ragchain/inference/graph.py`)
- **Function**: `query_rewriter()`
- **Purpose**: Enhance failed queries for better retrieval
- **LLM Model**: `get_llm(purpose="rewriting")`
- **Settings**: 
  - `temperature=0.5` (creative)
  - `num_predict=128` (moderate output)
  - `num_ctx=config.ollama_rewriting_ctx`
- **Prompt**: `QUERY_REWRITER_PROMPT`
- **When Called**: When retrieval grading fails (max 1 retry)
- **Output**: Enhanced query string

```python
llm = get_llm(purpose="rewriting")
prompt = QUERY_REWRITER_PROMPT.format(query=original)
rewritten = llm.invoke(prompt).strip()
```

---

### 3. **Answer Generator** (`src/ragchain/cli.py` & `src/ragchain/evaluation/judge.py`)
- **Function**: `cli.ask()` and `evaluate_questions()`
- **Purpose**: Generate natural language answers from retrieved context
- **LLM Model**: `get_llm(model=model, purpose="generation")`
- **Settings**: 
  - `temperature=0.1` (mostly deterministic)
  - `num_predict=1024` (long output)
  - `num_ctx=config.ollama_gen_ctx`
  - `reasoning=True` (enables chain-of-thought)
- **Prompt**: `RAG_ANSWER_TEMPLATE`
- **When Called**: After retrieval, to generate final answer
- **Output**: Natural language answer

```python
llm = get_llm(model=model, purpose="generation")
prompt = ChatPromptTemplate.from_template(RAG_ANSWER_TEMPLATE)
answer = llm.invoke(prompt.format(context=context, question=query))
```

---

### 4. **LLM-as-Judge** (`src/ragchain/evaluation/judge.py`)
- **Function**: `judge_answer()`
- **Purpose**: Evaluate answer quality (correctness, relevance, faithfulness)
- **LLM Model**: `get_llm(model=model, purpose="judging")`
- **Settings**: 
  - `temperature=0.0` (deterministic)
  - `num_predict=512` (JSON output)
  - `num_ctx=config.ollama_judging_ctx`
- **Prompt**: `JUDGE_PROMPT`
- **When Called**: During evaluation (`ragchain evaluate` command)
- **Output**: JSON scores (1-5 scale for 3 dimensions)

```python
llm = get_llm(model=model, purpose="judging")
prompt = ChatPromptTemplate.from_template(JUDGE_PROMPT)
raw_response = llm.invoke(judge_input)
evaluation = json.loads(raw_response.strip())
```

---

## ğŸš« Where LLMs Are NOT Called

### Statistical Grading (No LLM)
- **File**: `src/ragchain/inference/grader.py`
- **Function**: `grade_with_statistics()`
- **Method**: Keyword overlap + term frequency scoring
- **Reason**: Fast, cost-free, effective for relevance checking

---

## ğŸ“Š Architecture Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERACTION (CLI Commands)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                       â”‚                       â”‚
                â–¼                       â–¼                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  ragchain     â”‚      â”‚  ragchain     â”‚      â”‚  ragchain     â”‚
        â”‚  ingest       â”‚      â”‚  search       â”‚      â”‚  ask          â”‚
        â”‚               â”‚      â”‚               â”‚      â”‚               â”‚
        â”‚  (No LLM)     â”‚      â”‚  (No LLM)     â”‚      â”‚  (Full RAG)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                       â”‚                      â”‚
                â–¼                       â–¼                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Load & Chunk  â”‚      â”‚  Ensemble     â”‚      â”‚   RAG GRAPH       â”‚
        â”‚  Documents    â”‚      â”‚  Retrieval    â”‚      â”‚   (LangGraph)     â”‚
        â”‚               â”‚      â”‚  (BM25+Chroma)â”‚      â”‚                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                       â”‚                      â”‚
                â–¼                       â”‚                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚                      â”‚
        â”‚  Embed &      â”‚              â”‚                      â”‚
        â”‚  Store to     â”‚              â”‚                      â”‚
        â”‚  Chroma DB    â”‚              â”‚                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚                      â”‚
                                       â”‚                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                                  â”‚
â”‚                        FULL RAG PIPELINE (LangGraph Flow)                       â”‚
â”‚                                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  STEP 1: INTENT ROUTER                                                   â”‚  â”‚
â”‚  â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                 â”‚  â”‚
â”‚  â”‚  ğŸ“ LLM CALL #1: Intent Classification                                   â”‚  â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â”‚  â”‚
â”‚  â”‚  â€¢ File: src/ragchain/inference/router.py                                â”‚  â”‚
â”‚  â”‚  â€¢ Function: intent_router()                                             â”‚  â”‚
â”‚  â”‚  â€¢ Purpose: Classify query type                                          â”‚  â”‚
â”‚  â”‚  â€¢ Model: get_llm(purpose="routing")                                     â”‚  â”‚
â”‚  â”‚  â€¢ Config: temp=0.0, num_predict=32                                      â”‚  â”‚
â”‚  â”‚  â€¢ Prompt: INTENT_ROUTER_PROMPT                                          â”‚  â”‚
â”‚  â”‚  â€¢ Input: User query                                                     â”‚  â”‚
â”‚  â”‚  â€¢ Output: FACT | CONCEPT | COMPARISON                                   â”‚  â”‚
â”‚  â”‚                                                                           â”‚  â”‚
â”‚  â”‚  Fast-path: Simple "What is X?" queries skip LLM                         â”‚  â”‚
â”‚  â”‚  Feature flag: ENABLE_INTENT_ROUTING=false â†’ skip                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                    â†“                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  STEP 2: ADAPTIVE RETRIEVER                                              â”‚  â”‚
â”‚  â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                             â”‚  â”‚
â”‚  â”‚  ğŸ” NO LLM - Uses Statistical Retrieval                                  â”‚  â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚  â”‚
â”‚  â”‚  â€¢ File: src/ragchain/inference/graph.py                                 â”‚  â”‚
â”‚  â”‚  â€¢ Function: adaptive_retriever()                                        â”‚  â”‚
â”‚  â”‚  â€¢ Method: Ensemble (BM25 + Chroma vector search)                        â”‚  â”‚
â”‚  â”‚  â€¢ Uses: get_ensemble_retriever() with intent-specific weights:          â”‚  â”‚
â”‚  â”‚    - FACT: 80% BM25, 20% Chroma (keyword-heavy)                          â”‚  â”‚
â”‚  â”‚    - CONCEPT: 40% BM25, 60% Chroma (semantic-heavy)                      â”‚  â”‚
â”‚  â”‚    - COMPARISON: 50% BM25, 50% Chroma (balanced)                         â”‚  â”‚
â”‚  â”‚  â€¢ Algorithm: Reciprocal Rank Fusion (RRF)                               â”‚  â”‚
â”‚  â”‚  â€¢ Output: List of Document objects                                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                    â†“                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  STEP 3: RETRIEVAL GRADER                                                â”‚  â”‚
â”‚  â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                 â”‚  â”‚
â”‚  â”‚  ğŸ“Š NO LLM - Uses Statistical Scoring                                    â”‚  â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚  â”‚
â”‚  â”‚  â€¢ File: src/ragchain/inference/grader.py                                â”‚  â”‚
â”‚  â”‚  â€¢ Function: grade_with_statistics()                                     â”‚  â”‚
â”‚  â”‚  â€¢ Method:                                                               â”‚  â”‚
â”‚  â”‚    1. Extract keywords from query and docs                               â”‚  â”‚
â”‚  â”‚    2. Calculate overlap ratio (Jaccard similarity)                       â”‚  â”‚
â”‚  â”‚    3. Calculate term frequency (TF)                                      â”‚  â”‚
â”‚  â”‚    4. Score = 0.7 Ã— overlap + 0.3 Ã— TF                                   â”‚  â”‚
â”‚  â”‚    5. Check if top-3 docs have score â‰¥ 0.25                              â”‚  â”‚
â”‚  â”‚  â€¢ Output: YES | NO (GradeSignal)                                        â”‚  â”‚
â”‚  â”‚                                                                           â”‚  â”‚
â”‚  â”‚  Fast-paths:                                                             â”‚  â”‚
â”‚  â”‚  â€¢ ENABLE_GRADING=false â†’ Always YES                                     â”‚  â”‚
â”‚  â”‚  â€¢ No docs â†’ Always YES                                                  â”‚  â”‚
â”‚  â”‚  â€¢ Already retried â†’ Always YES (prevent loops)                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                    â†“                                            â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                      â”‚    Grade Result?          â”‚                              â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                    â”‚                                            â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚              â”‚ YES                 â”‚                     â”‚ NO                   â”‚
â”‚              â”‚                     â”‚                     â”‚                      â”‚
â”‚              â–¼                     â–¼                     â–¼                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚     â”‚  END (Success) â”‚    â”‚  Already Retry â”‚    â”‚ QUERY REWRITER â”‚             â”‚
â”‚     â”‚  Documents OK  â”‚    â”‚  retry_countâ‰¥1 â”‚    â”‚  retry_count=0 â”‚             â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                    â”‚                      â”‚                     â”‚
â”‚                                    â–¼                      â–¼                     â”‚
â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚                            â”‚  END (Give up) â”‚    â”‚  STEP 4: QUERY_REWRITERâ”‚    â”‚
â”‚                            â”‚  Accept anyway â”‚    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚   â”‚
â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  ğŸ“ LLM CALL #2:        â”‚   â”‚
â”‚                                                  â”‚     Query Enhancement   â”‚    â”‚
â”‚                                                  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚    â”‚
â”‚                                                  â”‚  â€¢ File: graph.py       â”‚    â”‚
â”‚                                                  â”‚  â€¢ Function:            â”‚    â”‚
â”‚                                                  â”‚    query_rewriter()     â”‚    â”‚
â”‚                                                  â”‚  â€¢ Model: get_llm(      â”‚    â”‚
â”‚                                                  â”‚    purpose="rewriting") â”‚    â”‚
â”‚                                                  â”‚  â€¢ Config: temp=0.5,    â”‚    â”‚
â”‚                                                  â”‚    num_predict=128      â”‚    â”‚
â”‚                                                  â”‚  â€¢ Prompt:              â”‚    â”‚
â”‚                                                  â”‚    QUERY_REWRITER_PROMPTâ”‚    â”‚
â”‚                                                  â”‚  â€¢ Input: Original queryâ”‚    â”‚
â”‚                                                  â”‚  â€¢ Output: Enhanced     â”‚    â”‚
â”‚                                                  â”‚    query string         â”‚    â”‚
â”‚                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                           â”‚                     â”‚
â”‚                                                           â–¼                     â”‚
â”‚                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚                                              â”‚  RETRY: Go back to     â”‚         â”‚
â”‚                                              â”‚  ADAPTIVE_RETRIEVER    â”‚         â”‚
â”‚                                              â”‚  (with rewritten query)â”‚         â”‚
â”‚                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                           â”‚                     â”‚
â”‚                                              (Loop back to STEP 2, max 1 time)  â”‚
â”‚                                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
                                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  STEP 5: ANSWER GENERATION                                â”‚
        â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                â”‚
        â”‚  ğŸ“ LLM CALL #3: Generate Natural Language Answer         â”‚
        â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
        â”‚  â€¢ File: src/ragchain/cli.py (ask command)                â”‚
        â”‚  â€¢ Function: cli.ask()                                    â”‚
        â”‚  â€¢ Model: get_llm(model=model, purpose="generation")      â”‚
        â”‚  â€¢ Config: temp=0.1, num_predict=1024, reasoning=True     â”‚
        â”‚  â€¢ Prompt: RAG_ANSWER_TEMPLATE                            â”‚
        â”‚  â€¢ Input: Retrieved documents + user query                â”‚
        â”‚  â€¢ Output: Natural language answer                        â”‚
        â”‚                                                           â”‚
        â”‚  Rules:                                                   â”‚
        â”‚  â€¢ ONLY use information from retrieved context            â”‚
        â”‚  â€¢ Say "I don't know" if context insufficient             â”‚
        â”‚  â€¢ Direct quotes preferred over summaries                 â”‚
        â”‚  â€¢ 150-300 word answers                                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
                                    â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  Return Answerâ”‚
                            â”‚  to User      â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â”‚ (Optional: Evaluation)
                                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  STEP 6: LLM-AS-JUDGE (Optional - ragchain evaluate)     â”‚
        â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•         â”‚
        â”‚  ğŸ“ LLM CALL #4: Evaluate Answer Quality                  â”‚
        â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
        â”‚  â€¢ File: src/ragchain/evaluation/judge.py                 â”‚
        â”‚  â€¢ Function: judge_answer()                               â”‚
        â”‚  â€¢ Model: get_llm(model=model, purpose="judging")         â”‚
        â”‚  â€¢ Config: temp=0.0, num_predict=512                      â”‚
        â”‚  â€¢ Prompt: JUDGE_PROMPT                                   â”‚
        â”‚  â€¢ Input: Question + Context + Answer                     â”‚
        â”‚  â€¢ Output: JSON with 3 scores (1-5 scale):                â”‚
        â”‚    - correctness: Factual accuracy                        â”‚
        â”‚    - relevance: Answers the question                      â”‚
        â”‚    - faithfulness: No hallucinations                      â”‚
        â”‚                                                           â”‚
        â”‚  Used by: ragchain evaluate command                       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¢ LLM Call Summary

| # | Component | File | Function | Purpose | Model Config | When Called |
|---|-----------|------|----------|---------|--------------|-------------|
| **1** | Intent Router | `router.py` | `intent_router()` | Query classification | `temp=0.0, predict=32` | Start of RAG (unless fast-path) |
| **2** | Query Rewriter | `graph.py` | `query_rewriter()` | Query enhancement | `temp=0.5, predict=128` | After grading fails (max 1Ã—) |
| **3** | Answer Generator | `cli.py`, `judge.py` | `ask()`, `evaluate_questions()` | Generate answer | `temp=0.1, predict=1024` | After retrieval completes |
| **4** | LLM-as-Judge | `judge.py` | `judge_answer()` | Answer evaluation | `temp=0.0, predict=512` | During evaluation only |

---

## ğŸ¯ Key Design Decisions

### Why Some Steps Use LLM, Others Don't

1. **Intent Router (LLM)**: 
   - Complex semantic understanding needed
   - Fast-path optimization for simple queries
   - Can be disabled (`ENABLE_INTENT_ROUTING=false`)

2. **Retrieval Grader (NO LLM)**:
   - Statistical scoring is fast & free
   - Keyword overlap sufficient for relevance
   - Prevents unnecessary API calls

3. **Query Rewriter (LLM)**:
   - Requires creative rephrasing
   - Only called on failure (rare)
   - Higher temperature (0.5) for creativity

4. **Answer Generator (LLM)**:
   - Core RAG functionality
   - Requires natural language synthesis
   - Reasoning mode enabled

5. **Judge (LLM)**:
   - Evaluation only (not production path)
   - Requires nuanced quality assessment
   - Runs in batch mode

---

## ğŸš€ Optimization Strategies

### Performance Optimizations
1. **Fast-path routing**: Simple queries skip LLM classification
2. **Grading can be disabled**: `ENABLE_GRADING=false`
3. **Max 1 retry**: Prevents infinite loops
4. **Token limits**: All LLMs have `num_predict` caps
5. **Statistical grading**: No LLM cost for quality checks

### Cost Optimizations
1. **Purpose-specific models**: Different context windows per use case
2. **Conditional LLM calls**: Routing/rewriting only when needed
3. **Caching**: Ensemble retriever caches results
4. **Parallel retrieval**: BM25 + Chroma run concurrently

---

## ğŸ“ Configuration Options

All LLM configurations are centralized in `src/ragchain/utils.py::get_llm()`:

```python
purpose_defaults = {
    "generation":  {"temperature": 0.1, "num_ctx": 8192, "num_predict": 1024, "reasoning": True},
    "routing":     {"temperature": 0.0, "num_ctx": 4096, "num_predict": 32,   "reasoning": False},
    "judging":     {"temperature": 0.0, "num_ctx": 4096, "num_predict": 512,  "reasoning": False},
    "rewriting":   {"temperature": 0.5, "num_ctx": 4096, "num_predict": 128,  "reasoning": False},
}
```

---

## ğŸ” Tracing LLM Calls in Code

All LLM invocations follow this pattern:

```python
# 1. Get LLM with purpose-specific config
llm = get_llm(purpose="routing")  # or "generation", "judging", "rewriting"

# 2. Format prompt
prompt = SOME_PROMPT_TEMPLATE.format(query=user_query)

# 3. Invoke LLM
response = llm.invoke(prompt)

# 4. Process response
result = response.strip()  # or json.loads(response), etc.
```

Search for `.invoke(` to find all LLM calls:
```bash
rg "\.invoke\(" --type py src/ragchain/
```

---

## ğŸ“š Related Documentation

- **Full codebase walkthrough**: `docs/codewalk.md`
- **LangGraph details**: `docs/langGraph.md`
- **Advanced RAG strategies**: `docs/advanceRAG_strategies.md`
- **Project overview**: `AGENTS.md`

---

## ğŸ“ Learning Resources

**Key LangChain Concepts Used:**
- `OllamaLLM`: LLM wrapper for local Ollama models
- `ChatPromptTemplate`: Prompt formatting
- `StateGraph`: LangGraph state management
- `EnsembleRetriever`: Custom RRF retrieval

**External Dependencies:**
- **Ollama**: Local LLM runtime
- **Chroma**: Vector database
- **LangChain**: RAG orchestration framework
- **LangGraph**: Agentic workflow framework

---

## ğŸ“Š Example Flow: "What is Python?"

```
1. User: "What is Python?"
   â†“
2. Intent Router (LLM Call #1)
   - Fast-path detected: "what is" pattern
   - Skip LLM, return: CONCEPT
   â†“
3. Adaptive Retriever (No LLM)
   - CONCEPT â†’ 40% BM25, 60% Chroma
   - Retrieve 6 documents
   â†“
4. Retrieval Grader (No LLM)
   - Extract keywords: {"python"}
   - Score docs: [0.85, 0.72, 0.68, ...]
   - Top doc score â‰¥ 0.25 â†’ YES
   â†“
5. Answer Generator (LLM Call #2)
   - Context: 6 document chunks
   - Prompt: RAG_ANSWER_TEMPLATE
   - Output: "Python is a high-level, interpreted..."
   â†“
6. Return answer to user
```

**Total LLM Calls**: **1** (skipped intent routing via fast-path)

---

## ğŸ“Š Example Flow: "Compare Go and Rust" (with retry)

```
1. User: "Compare Go and Rust"
   â†“
2. Intent Router (LLM Call #1)
   - LLM classifies: COMPARISON
   â†“
3. Adaptive Retriever (No LLM)
   - COMPARISON â†’ 50% BM25, 50% Chroma
   - Retrieve 6 documents
   - Assume: Got only Go docs, no Rust
   â†“
4. Retrieval Grader (No LLM)
   - Keywords: {"compare", "rust"}
   - Top doc score: 0.18 < 0.25 â†’ NO
   â†“
5. Query Rewriter (LLM Call #2)
   - Original: "Compare Go and Rust"
   - Rewritten: "Go programming language features performance Rust programming..."
   - retry_count = 1
   â†“
6. Adaptive Retriever (Retry, No LLM)
   - Use rewritten query
   - Retrieve 6 documents
   - Now has both Go AND Rust docs
   â†“
7. Retrieval Grader (No LLM)
   - Top doc score: 0.67 â‰¥ 0.25 â†’ YES
   â†“
8. Answer Generator (LLM Call #3)
   - Context: 6 mixed Go/Rust docs
   - Output: "Go and Rust are both systems languages..."
   â†“
9. Return answer to user
```

**Total LLM Calls**: **3** (routing + rewriting + generation)

---

## ğŸ”§ Debugging LLM Calls

Enable debug logging to see all LLM interactions:

```bash
export LOG_LEVEL=DEBUG
uv run ragchain ask "Your query"
```

Look for these log patterns:
- `[intent_router]` â†’ Intent classification
- `[query_rewriter]` â†’ Query enhancement
- `[adaptive_retriever]` â†’ Document retrieval (no LLM)
- `[retrieval_grader]` â†’ Quality check (no LLM)
- Final answer generation (in CLI output)

---

## ğŸ“Œ Summary

**Total LLM Call Points: 4**
1. Intent Router (conditional, can skip)
2. Query Rewriter (conditional, only on failure)
3. Answer Generator (always)
4. LLM-as-Judge (evaluation only)

**Per Query Typical LLM Usage:**
- **Minimum**: 1 call (fast-path: skip routing, no retry)
- **Average**: 2 calls (routing + generation)
- **Maximum**: 3 calls (routing + rewriting + generation)
- **Evaluation**: +1 call (judging, separate workflow)

**Cost-Saving Features:**
- Statistical grading (no LLM for quality checks)
- Fast-path routing (skip LLM for simple queries)
- Conditional rewriting (only on failure)
- Token limits (prevent runaway generation)
- Feature flags (disable routing/grading)

---

**Last Updated**: 2026-02-07
**Author**: Generated from codebase analysis
**Version**: 1.0.0
