# AGENTS â€” Project Overview

This document summarizes the code layout under `src/ragchain`, key configuration in `pyproject.toml`, and the testing/CLI/tooling conventions used in this repository. It is intended for contributors and CI to quickly understand where things live and what to configure.

---

## ğŸ“ Repository layout (src/ragchain)

A compact tree view of the repository layout:

```
src/ragchain/
â”œâ”€â”€ cli.py                # Click-based CLI (ingest, search, ask, evaluate)
â”œâ”€â”€ prompts.py            # LLM prompt templates
â”œâ”€â”€ config.py             # Configuration management (singleton)
â”œâ”€â”€ types.py              # Shared enums and TypedDicts
â”œâ”€â”€ utils.py              # Utility functions for logging, timing, and other helpers
â”œâ”€â”€ evaluation/           # Answer generation and evaluation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ judge.py          # LLM-as-judge evaluation for RAG answers
â”œâ”€â”€ ingestion/            # Document loading and storage
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loaders.py        # Document loaders for Wikipedia and other sources
â”‚   â””â”€â”€ storage.py        # Storage utilities: embeddings, vector store, document ingestion
â”œâ”€â”€ inference/            # Retrieval, routing, and orchestration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ graph.py          # LangGraph intent-based adaptive RAG orchestration
â”‚   â”œâ”€â”€ rag.py            # RAG search orchestration
â”‚   â”œâ”€â”€ retrievers.py     # Retrieval utilities: ensemble retriever and helpers
â”‚   â”œâ”€â”€ router.py         # Intent routing logic
â”‚   â””â”€â”€ grader.py         # Document relevance grading
â””â”€â”€ __init__.py           # Package initialization
```

**Key architectural notes:**

- **`data/config.py`** provides centralized configuration management:
  - Singleton `Config` class for environment variable handling
  - Typed attributes for Ollama models, Chroma settings, and feature flags
  - Used throughout the codebase for consistent configuration access

- **`inference/rag.py`** is the RAG search orchestration:
  - `search()` â€” Ensemble retrieval using BM25 and Chroma vector search

- **`ingestion/storage.py`** handles storage and ingestion:
  - `get_embedder()` â€” Creates OllamaEmbeddings with `qwen3-embedding:4b` model for 2560-dimensional vectors with 4k context
  - `get_vector_store()` â€” Returns Chroma (local persistent or remote HTTP) with LangChain integration
  - `ingest_documents()` â€” Fetches documents â†’ parses â†’ chunks recursively â†’ embeds â†’ upserts to vector store

- **`inference/retrievers.py`** provides retrieval logic:
  - `EnsembleRetriever` â€” Custom retriever implementing Reciprocal Rank Fusion (RRF) with configurable weights
  - `get_ensemble_retriever()` â€” Factory with intent-specific weight support

- **`inference/graph.py`** is the agentic orchestrator using LangGraph:
  - `IntentRoutingState` â€” Typed state management for the RAG graph
  - `intent_router()` â€” LLM-based query classification (FACT/CONCEPT/COMPARISON)
  - `adaptive_retriever()` â€” Retrieves with intent-specific BM25/Chroma weights
  - `retrieval_grader()` â€” LLM-based validation of document relevance
  - `query_rewriter()` â€” Enhances queries on retrieval failure for automatic retry
  - `rag_graph` â€” Compiled LangGraph with conditional retry logic

- **`prompts.py`** contains prompt templates:
  - `RAG_ANSWER_TEMPLATE` â€” Answer generation from context
  - `INTENT_ROUTER_PROMPT` â€” Query classification
  - `RETRIEVAL_GRADER_PROMPT` â€” Document relevance validation
  - `QUERY_REWRITER_PROMPT` â€” Query enhancement

- **`ingestion/loaders.py`** provides document loading utilities:
  - Wikipedia article fetching (via built-in Wikipedia API or custom parsers)
  - Extensible for other sources (local files, APIs, etc.)

- **`cli.py`** provides Click-based commands:
  - `ingest` â€” Load documents into vector store
  - `search` â€” Semantic search over ingested documents
  - `ask` â€” Intent-based adaptive RAG with LLM generation
  - `evaluate` â€” LLM-as-judge evaluation framework

- **`utils.py`** provides logger helpers to simplify the monitoring experience, including:
  - `log_with_prefix()` â€” Logs messages with a consistent prefix for easier filtering

- Supports both **local persistent Chroma** (`CHROMA_PERSIST_DIRECTORY`) and **remote HTTP Chroma** (`CHROMA_SERVER_URL`)
- Uses **ensemble retrieval** with Reciprocal Rank Fusion (RRF) combining BM25 keyword search and semantic vector search
- Implements **intent-based adaptive RAG** via LangGraph:
  - FACT queries: 0.8 BM25 / 0.2 Chroma (keyword-heavy for enumerations)
  - CONCEPT queries: 0.4 BM25 / 0.6 Chroma (balanced)
  - COMPARISON queries: 0.3 BM25 / 0.7 Chroma (semantic-heavy)
- **Self-correcting**: Automatically rewrites and re-retrieves if grading fails (max 1 retry)
- Tests use deterministic embeddings and mock external HTTP where possible (using `aioresponses`)

---

## ğŸ§° Tooling and configuration (`pyproject.toml` highlights)

**Runtime dependencies:**

- **LangChain ecosystem** â€” LangChain, LangChain-Community, LangChain-Ollama, LangChain-Chroma for unified RAG orchestration
- **LangGraph** â€” `langgraph` for agentic RAG orchestration with state management and conditional routing
- **Ollama integration** â€” `langchain-ollama` for embedding (`qwen3-embedding:4b`) and LLM generation (`qwen3:8b`)
- **Vector store** â€” `chromadb` for semantic search (supports local persistent and remote HTTP)
- **BM25** â€” `rank-bm25` for keyword-based retrieval and ensemble ranking
- **Click** â€” CLI framework for data operations and queries
- **Pydantic Settings** â€” Environment configuration management
- **Data fetching** â€” `aiohttp` for async HTTP, `beautifulsoup4` + `wikipedia` for document loading

**Developer tooling** (installed via `uv sync`):

- **Ruff** (linter & formatter) â€” `line-length = 160`
- **mypy** â€” static type checking (configured to ignore missing imports)
- **pytest** + **pytest-asyncio** â€” testing framework with integration markers
- **aioresponses** â€” mock async HTTP requests in tests

**Project entry points:**

- `ragchain` console script â†’ `ragchain.cli:cli` (enables `ragchain ingest`, `ragchain search`, `ragchain ask`, etc.)

**Recommended Python version:** **3.12** (LangChain ecosystem has optimized wheels)

---

## âš™ï¸ Environment Variables

The following environment variables can be used to configure the RAGChain system:

**Vector Store Configuration:**

- `CHROMA_PERSIST_DIRECTORY` â€” Directory for local Chroma persistence (default: `./chroma_data`)
- `CHROMA_SERVER_URL` â€” URL for remote Chroma server (default: empty string for local storage; set to `http://localhost:8000` to use remote Chroma)

**Ollama Configuration:**

- `OLLAMA_BASE_URL` â€” Base URL for Ollama API (default: `http://localhost:11434`)
- `OLLAMA_EMBED_MODEL` â€” Model name for embeddings (default: `qwen3-embedding:4b`)
- `OLLAMA_MODEL` â€” Model name for text generation (default: `qwen3:8b`)
- `OLLAMA_EMBED_CTX` â€” Context window size for embedding model (default: `4096`)
- `OLLAMA_GEN_CTX` â€” Context window size for generation model (default: `8192`)

**Document Processing:**

- `CHUNK_SIZE` â€” Size of document chunks in characters (default: `2500`)
- `CHUNK_OVERLAP` â€” Overlap between chunks in characters (default: `500`)

**Retrieval Configuration:**

- `SEARCH_K` â€” Number of documents to retrieve per retriever for search API (default: `10`)
- `GRAPH_K` â€” Number of documents for graph-based RAG pipeline (default: `6`)
- `RRF_MAX_RESULTS` â€” Maximum results to return after RRF fusion (default: `10`)

**Feature Flags:**

- `ENABLE_GRADING` â€” Enable/disable document relevance grading (default: `true`)
- `ENABLE_INTENT_ROUTING` â€” Enable/disable intent-based routing (default: `true`)

---

## ğŸ§ª Running tests and remote Chroma

**Unit tests** (using mocked dependencies):

```bash
uv run --with-editable . pytest -q
```

**Integration tests** against a running local Chroma service:

```bash
# Start Ollama (if not already running)
ollama serve

# Run full pipeline integration tests (uses local Chroma persistence)
CHROMA_SERVER_URL= uv run --with-editable . pytest -m integration
```

**Local development:**

- `docker compose up -d` â€” Starts Chroma vector database
- `uv run ragchain ingest` â€” Ingest all 50 programming languages + 10 conceptual bridge pages
- `uv run ragchain search "Python programming"` â€” Search ingested documents
- `uv run ragchain ask "What is Python?"` â€” Ask questions with RAG + LLM
- `uv run ragchain evaluate` â€” Run LLM-as-judge evaluation

**Stack components:**

- **Chroma** (vector database) â€” `http://localhost:8000`

---

## ğŸ”§ Notes & Rationale

- **LangGraph agentic RAG** â€” Intent-aware routing adapts retrieval weights for FACT/CONCEPT/COMPARISON queries
- **Reciprocal Rank Fusion** â€” Principled ensemble ranking (score = 1/(rank+60)) combining BM25 keyword and semantic search
- **Self-correcting** â€” Automatic query rewriting on retrieval failure (max 1 retry) with LLM-based relevance grading
- **Performance optimized** â€” Parallel retrieval (ThreadPoolExecutor), retriever caching, optional grading, and fast-path routing
- **qwen3-embedding:4b model** â€” 2560-dimensional embeddings with 4k context window for semantic search (via Ollama)
- **Flexible & composable** â€” Supports local/remote Chroma storage; easily swappable embedders, vector stores, and LLM models via config
- **Deterministic testing** â€” Mock HTTP (aioresponses) enables testing without Ollama/Chroma servers; Docker profiles for CI/demo
- **Local-first architecture** â€” Defaults to local persistent storage, no Docker required for basic usage
